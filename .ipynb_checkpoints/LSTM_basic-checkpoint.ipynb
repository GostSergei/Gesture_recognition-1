{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42db68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9970a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2394327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9834eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e253d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e49f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to online \n",
    "def shift_scale_sample_4d(data, copy_data=True, do_flip=True):\n",
    "    P_SHOULDER_LEFT, P_SHOULDER_RIGHT = 11, 12\n",
    "    P_HIP_LEFT, P_HIP_RIGHT = 23, 24\n",
    "    K_HEIGHT_TO_WIDTH = 1\n",
    "    \n",
    "    if copy_data:\n",
    "        data = data.copy()\n",
    "    \n",
    "    # 0) flip Y\n",
    "    if do_flip:\n",
    "        data[:, :, :, 1] *= -1\n",
    "\n",
    "    # 1) shifting\n",
    "    origin_shoulders = 0.5*(data[:, :, P_SHOULDER_LEFT:P_SHOULDER_LEFT+1, :2] + data[:, :, P_SHOULDER_RIGHT:P_SHOULDER_RIGHT+1, :2] )\n",
    "    # origin_hips = 0.5*(data[:, :, P_HIP_LEFT:P_HIP_LEFT+1, :2] + data[:, :, P_HIP_RIGHT:P_HIP_RIGHT+1, :2] )\n",
    "    data = data - origin_shoulders\n",
    "\n",
    "    # 2) scaling\n",
    "    kx = 1 / data[:, :, P_SHOULDER_LEFT:P_SHOULDER_LEFT+1, 0]\n",
    "    origin_hips_new = 0.5*(data[:, :, P_HIP_LEFT:P_HIP_LEFT+1, :2] + data[:, :, P_HIP_RIGHT:P_HIP_RIGHT+1, :2] )\n",
    "    # origin_shoulders_new = 0.5*(data[:, :, P_SHOULDER_LEFT:P_SHOULDER_LEFT+1, :2] + data[:, :, P_SHOULDER_RIGHT:P_SHOULDER_RIGHT+1, :2] )\n",
    "\n",
    "    ky = K_HEIGHT_TO_WIDTH / origin_hips_new[:, :, :, 1]\n",
    "    data[:, :, :, 0] *=  kx\n",
    "    data[:, :, :, 1] *=  -ky\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6688b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tensor_by_se(tensor_data, se_list, s_e_new=[5, 65], filling='fl'):\n",
    "    \n",
    "    good_fillings = ['fl', 'first/last', 'roll']\n",
    "    assert filling in good_fillings, f\"Error! filling={filling} should be in {good_fillings}!\"\n",
    "    \n",
    "    f_tensor = tensor_data.copy()\n",
    "    samples, duration, signals = f_tensor.shape\n",
    "    \n",
    "    s_e_new = np.asarray(s_e_new).reshape(-1, 2)\n",
    "    \n",
    "    if s_e_new.shape[0] == 1:\n",
    "        s_e_new = np.repeat(s_e_new, samples, axis=0)\n",
    "    elif s_e_new.shape[0] == samples:\n",
    "        pass\n",
    "    else:\n",
    "        assert 0, f\"Error, s_e_new.shape={s_e_new.shape} in np.array from\"\\\n",
    "                  f\" should have 1 or {samples} size of the first dimension\"\\\n",
    "                  f\"for given 'tensor_data'[{tensor_data.shape}]\"\n",
    "    \n",
    "    print(s_e_new.shape)\n",
    "    se_array = np.asarray(se_list)\n",
    "    t = np.arange(0, duration)\n",
    "    t_tensor = np.repeat(t.reshape(1, -1), samples, axis=0)\n",
    "    \n",
    "    \n",
    "    k_stretching =  (s_e_new [:,1:2] - s_e_new [:,0:1])/(se_array[:,1:2] - se_array[:,0:1])   \n",
    "    t_tensor_  = (t_tensor -  se_array[:, 0:1])*k_stretching + s_e_new[:,0:1]\n",
    "    \n",
    "    \n",
    "    # Skipping the null procedures\n",
    "    mask = (np.round(s_e_new,0) == np.round(se_array,0)).prod(axis=1).astype(bool)\n",
    "    # print(mask)\n",
    "    t_tensor_[mask] = t_tensor[mask]\n",
    "    print(f\"{mask.sum()} procedures (from {mask.size}) were skipped!\")\n",
    "    \n",
    "    t_tensor = t_tensor_\n",
    "    \n",
    "    interp_period = None\n",
    "    if filling in ['fl', 'first/last']:\n",
    "        interp_period = None\n",
    "    elif filling in ['roll']:\n",
    "        interp_period = duration\n",
    "    else:\n",
    "        print(f\"Warning! activity for 'filling'={filling} is not defined! Default: interp_period=None!!\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    for k in range(signals):\n",
    "        for i in range(samples):\n",
    "            f = f_tensor[i, :, k]\n",
    "            t = t_tensor[i, :]\n",
    "            f_new = np.interp(np.arange(0, duration),t, f, period=interp_period)\n",
    "            f_tensor[i, :, k] = f_new\n",
    "            \n",
    "    return f_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22747dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_4d__ = shift_scale_sample_4d(data_4d, copy_data=True, do_flip=True)\n",
    "# data_4d__.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92417a9d",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18dfe9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_file_clean = loadmat('data/ALIGNED_SER_4d_full.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e99dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'labels', 'data', 'starts', 'stops'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_file_clean.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ebe1bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2, ..., 27, 42, 45], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_full = mt_file_clean['labels'][0]\n",
    "labels_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe716a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7420, 120, 67, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full = mt_file_clean['data']\n",
    "data_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b817aadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7420, 120, 67, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4d = data_full[::,::,::,0:2]\n",
    "data_4d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b918145c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7420, 120, 67, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # scaling\n",
    "data_4d = shift_scale_sample_4d(data_4d, copy_data=True, do_flip=True)\n",
    "data_4d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a12438cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20., 20., 20., ..., 20., 20., 20.]),\n",
       " array([76. , 63. , 76. , ..., 73. , 77.5, 59. ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts, stops = mt_file_clean['starts'][0], mt_file_clean['stops'][0]\n",
    "(starts, stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddd248",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81355856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7420, 120, 134)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3d_full = data_4d.reshape(data_4d.shape[0], data_4d.shape[1], data_4d.shape[2]*data_4d.shape[3])\n",
    "data_3d_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33346e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7420, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661bd7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7420, 2)\n",
      "(7420, 2)\n",
      "0 procedures (from 7420) were skipped!\n"
     ]
    }
   ],
   "source": [
    "# scaling\n",
    "se = np.stack([starts,stops], axis=1)\n",
    "print(se.shape)\n",
    "data_3d_full = align_tensor_by_se(data_3d_full, se, s_e_new=[5, 65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ee60d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6678, 120, 134)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3d_train, data_3d_test, labels_train, labels_test = train_test_split(data_3d_full, labels_full, test_size=0.1)\n",
    "\n",
    "data_3d_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5503a70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6678, 120, 134)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_2d_train = data_3d_train.reshape(data_3d_train.shape[0], data_3d_train.shape[1]*data_3d_train.shape[2])\n",
    "data_2d_test = data_3d_test.reshape(data_3d_test.shape[0], data_3d_test.shape[1]*data_3d_test.shape[2])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_2d_train)\n",
    "\n",
    "data_2d_train = scaler.transform(data_2d_train)\n",
    "data_2d_test = scaler.transform(data_2d_test)\n",
    "\n",
    "data_3d_train = data_2d_train.reshape(data_3d_train.shape[0], data_3d_train.shape[1], data_3d_train.shape[2])\n",
    "data_3d_test =  data_2d_test.reshape(data_3d_test.shape[0], data_3d_test.shape[1], data_3d_test.shape[2])\n",
    "\n",
    "data_3d_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92db70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset_LSTM_Single(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        data_pairs_list = []\n",
    "        for sig, label in zip(data, labels):\n",
    "            data_pairs_list.append([sig, label])\n",
    "            \n",
    "        self.data_pairs_list = data_pairs_list\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.uniq_labels = len(np.unique(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         gesture_seqence, label = self.data_pairs_list[idx]\n",
    "#         gesture_seqence = torch.tensor(gesture_seqence, dtype=torch.float)\n",
    "#         label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        gesture_seqence, label = self.data_pairs_list[idx]\n",
    "        # slicing\n",
    "        start_idx = np.random.randint(low=0, high=gesture_seqence.shape[0]-20)\n",
    "        gesture_seqence = gesture_seqence[start_idx: start_idx+20]\n",
    "        if start_idx < 10 or start_idx > gesture_seqence.shape[0]-30:\n",
    "            label = self.uniq_labels\n",
    "        \n",
    "        gesture_seqence = torch.tensor(gesture_seqence, dtype=torch.float)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return gesture_seqence, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b06453c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(low=0, high=data_3d_train[0].shape[0]-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9df0c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 134]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "DS_TRAIN = Dataset_LSTM_Single(data_3d_train, labels_train)\n",
    "DL_TRAIN = DataLoader(DS_TRAIN, batch_size=4, shuffle=False)\n",
    "\n",
    "DS_TEST = Dataset_LSTM_Single(data_3d_test, labels_test)\n",
    "DL_TEST = DataLoader(DS_TEST, batch_size=4, shuffle=False)\n",
    "\n",
    "for x, y in DL_TRAIN:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc9ba9",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61677843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.LSTM_simple_classifier import LSTMTagger\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5035e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No batchnorm\n",
      "dropout\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = LSTMTagger(N_input_features=134, hidden_dim=128, N_lstm_layers=2, dropout=0.1, # lstm\n",
    "                 N_1d_filters=3, kernel_size=5, # 1d convolution\n",
    "                 target_size=len(np.unique(labels_full))+1, use_all_lstm_layers=True, # last linear layer\n",
    "                 )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(LSTM_model.parameters(), lr=3e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afbb197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 54])\n"
     ]
    }
   ],
   "source": [
    "for seq, tags in DL_TRAIN:\n",
    "#     LSTM_model.zero_grad()\n",
    "    seq, tags = seq.to(device), tags.to(device)\n",
    "    pred_tags = LSTM_model(seq)\n",
    "    break\n",
    "\n",
    "print(pred_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa34489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, AUROC, F1Score, Precision, Recall, AveragePrecision\n",
    "from collections import Counter\n",
    "\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=len(Counter(labels_full)) + 1  ).to(device)\n",
    "# f1_scorer = F1Score(task=\"multiclass\", num_classes=len(Counter(labels_full))).to(device)\n",
    "# precision = Precision(task=\"multiclass\", num_classes=len(Counter(labels_full))).to(device)\n",
    "# recall = Recall(task=\"multiclass\", num_classes=len(Counter(labels_full))).to(device)\n",
    "\n",
    "def validate(model, dl_test, metric=accuracy):\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for seq, tags in dl_test:\n",
    "            seq, tags = seq.to(device), tags.to(device)\n",
    "            pred_tags = model(seq)\n",
    "            \n",
    "            res.append(metric(torch.argmax(pred_tags, dim=1), tags).item())\n",
    "    model.train(True)\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e2dc964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(labels_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f558fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [01:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     acc_test\u001b[38;5;241m.\u001b[39mappend(validate(LSTM_model, DL_TEST, accuracy))\n\u001b[0;32m     27\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "acc_list = []\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    count = 0\n",
    "    for seq, tags in DL_TRAIN:\n",
    "        LSTM_model.zero_grad()\n",
    "        seq, tags = seq.to(device), tags.to(device)\n",
    "        pred_tags = LSTM_model(seq)\n",
    "        \n",
    "#         print('tags', tags.shape)\n",
    "#         print('pred_tags', pred_tags.shape)\n",
    "        \n",
    "\n",
    "#         pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "\n",
    "        # Cross entropy input size is (N, C) N-batchsize, C-num_classes\n",
    "        # we can treat L(sequence length) as multiple of N\n",
    "        tags = tags.view(-1)\n",
    "        loss = loss_fn(pred_tags, tags)\n",
    "        \n",
    "        acc_list.append(accuracy(torch.argmax(pred_tags, dim=1), tags).item())\n",
    "    \n",
    "        if count % 500 == 0:\n",
    "            acc_test.append(validate(LSTM_model, DL_TEST, accuracy))\n",
    "        count += 1\n",
    "        \n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8039835",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31018725",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "RF = RandomForestClassifier(max_depth=10, n_estimators=50)\n",
    "LR = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e577a02",
   "metadata": {},
   "source": [
    "### 1. Right/wrong window classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded82269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_data_true = []\n",
    "window_data_false = []\n",
    "\n",
    "window_size = 20\n",
    "steps = np.arange(0,101,5)\n",
    "\n",
    "for strt in tqdm(steps):\n",
    "    data_3d_slice = data_3d_train[::,::,::]\n",
    "    data_2d = data_3d_slice[::,strt:strt+window_size,::].reshape(data_3d_slice.shape[0],\n",
    "                                                                 window_size*data_3d_slice.shape[2])\n",
    "    if strt >= 20 and strt <= 30:\n",
    "        window_data_true.append(data_2d)\n",
    "    else:\n",
    "        window_data_false.append(data_2d)\n",
    "        \n",
    "print(len(window_data_false))\n",
    "print(len(window_data_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ef72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_indx(coef=3/18):\n",
    "    return np.random.choice(window_data_true[0].shape[0], size=int(window_data_true[0].shape[0]*coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_data_array_true = np.concatenate(window_data_true)\n",
    "window_data_array_false = np.concatenate([x[rnd_indx()] for x in window_data_false])\n",
    "window_data_array = np.concatenate([window_data_array_true, window_data_array_false])\n",
    "window_labels = [1]*window_data_array_true.shape[0] + [0]*window_data_array_false.shape[0]\n",
    "\n",
    "\n",
    "window_data_array, window_labels = shuffle(window_data_array, window_labels, random_state=0)\n",
    "\n",
    "print(window_data_array.shape)\n",
    "print(len(window_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('LR', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# data_2d_window = scaler.fit_transform(window_data_array.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b71ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(estimator=pipe, X=window_data_array, y=window_labels)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9275dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_window_clf = Pipeline([('scaler', StandardScaler()), ('LR', LR)])\n",
    "LR_window_clf.fit(window_data_array, window_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_window_clf.pkl', 'wb') as f:\n",
    "    pickle.dump(LR_window_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdee199",
   "metadata": {},
   "source": [
    "### 2. Gesture classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea08b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_labels = np.concatenate([labels_train for i in range(len(window_data_true))])\n",
    "gesture_data = window_data_array_true\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('LR', LogisticRegression())])\n",
    "\n",
    "cv_scores = cross_val_score(estimator=pipe, X=gesture_data, y=gesture_labels)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40429282",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_gesture_clf = Pipeline([('scaler', StandardScaler()), ('LR', LogisticRegression())])\n",
    "LR_gesture_clf.fit(gesture_data, gesture_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59311efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LR_gesture_clf.pkl', 'wb') as f:\n",
    "    pickle.dump(LR_gesture_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0061be",
   "metadata": {},
   "source": [
    "### 3. Testing Window LDA + Gesture LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "window_size = 20\n",
    "steps = np.arange(0,101,1)\n",
    "\n",
    "for strt in tqdm(steps):\n",
    "    data_3d_slice = data_3d_test[::,::,::]\n",
    "    data_2d = data_3d_slice[::,strt:strt+window_size,::].reshape(data_3d_slice.shape[0],\n",
    "                                                                 window_size*data_3d_slice.shape[2])\n",
    "    \n",
    "    pred_labels = np.ones_like(labels_test)*-1\n",
    "    pred_window_mask = LR_window_clf.predict(data_2d)\n",
    "    if np.sum(pred_window_mask) > 0:\n",
    "        data_2d_masked = data_2d[pred_window_mask==1]\n",
    "        pred_labels_masked = LR_gesture_clf.predict(data_2d_masked)\n",
    "        pred_labels[pred_window_mask==1] = pred_labels_masked\n",
    "    \n",
    "    acc = accuracy_score(labels_test, pred_labels)\n",
    "    accuracy_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c11a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.title(f'LR window={window_size}')\n",
    "plt.plot(steps*2 + window_size//2, accuracy_list, 'r')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,110)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbee63",
   "metadata": {},
   "source": [
    "### Shifting window accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0945aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# labels = shuffle(labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb92b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_scores_mean = []\n",
    "lda_scores_std = []\n",
    "steps = np.arange(0,41,5)\n",
    "window_size = 20\n",
    "for strt in tqdm(steps):\n",
    "    data_3d_slice = data_3d[::,::2,::]\n",
    "    data_2d = data_3d_slice[::,strt:strt+window_size,::].reshape(data_3d_slice.shape[0],window_size*data_3d_slice.shape[2])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_2d = scaler.fit_transform(data_2d.T).T\n",
    "    \n",
    "    cv_scores = cross_val_score(estimator=LDA, X=data_2d, y=labels)\n",
    "    lda_scores_mean.append(np.mean(cv_scores))\n",
    "    lda_scores_std.append(np.std(cv_scores))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da739bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3d_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c20f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcba0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.title(f'LDA window={window_size}')\n",
    "plt.plot(steps*2 + window_size//2, np.array(lda_scores_mean) + 3*np.array(lda_scores_std), 'g', alpha=0.3)\n",
    "plt.plot(steps*2 + window_size//2, lda_scores_mean, 'r')\n",
    "plt.plot(steps*2 + window_size//2, np.array(lda_scores_mean) - 3*np.array(lda_scores_std), 'g', alpha=0.3)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,110)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226130d",
   "metadata": {},
   "source": [
    "### Consequitive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb601aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "\n",
    "data_3d_resh = np.concatenate([data_3d[::,20:40,::],\n",
    "                               ])\n",
    "\n",
    "print(data_3d_resh.shape)\n",
    "\n",
    "data_2d_resh = data_3d_resh.reshape(data_3d_resh.shape[0],\n",
    "                                    data_3d_resh.shape[1]*data_3d_resh.shape[2])\n",
    "print(data_2d_resh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcfd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_2d_resh = scaler.fit_transform(data_2d_resh.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(estimator=LDA,\n",
    "                            X=data_2d_resh,\n",
    "                            y=np.concatenate([labels]*1),\n",
    "                            )\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b4cde",
   "metadata": {},
   "source": [
    "### Sequential prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be84aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_3d, labels, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb771544",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "\n",
    "X1 = X_train[::,0:window_size,::].reshape(X_train.shape[0],window_size*X_train.shape[2])\n",
    "X2 = X_train[::,20:20+window_size,::].reshape(X_train.shape[0],window_size*X_train.shape[2])\n",
    "X3 = X_train[::,40:40+window_size,::].reshape(X_train.shape[0],window_size*X_train.shape[2])\n",
    "X4 = X_train[::,60:60+window_size,::].reshape(X_train.shape[0],window_size*X_train.shape[2])\n",
    "X5 = X_train[::,80:80+window_size,::].reshape(X_train.shape[0],window_size*X_train.shape[2])\n",
    "\n",
    "print(X_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda_model\n",
    "from sklearn.ensemble import RandomForestClassifier as lda_model\n",
    "LDA1, LDA2, LDA3, LDA4, LDA5 = lda_model(), lda_model(), lda_model(), lda_model(), lda_model()\n",
    "LDA1.fit(X1, y_train)\n",
    "LDA2.fit(X2, y_train)\n",
    "LDA3.fit(X3, y_train)\n",
    "LDA4.fit(X4, y_train)\n",
    "LDA5.fit(X5, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4add44",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2, p3, p4, p5 = [], [], [], [], []\n",
    "for X_slice in [X1, X2, X3, X4, X5]:\n",
    "    for model, p_list in zip([LDA1,LDA2,LDA3,LDA4,LDA5], [p1,p2,p3,p4,p5]):\n",
    "        p_list.append(model.predict_proba(X_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6729409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_train, np.argmax((p1[0] + p2[0] + p3[0] + p4[0] + p5[0])/5,  axis=1)))\n",
    "print(accuracy_score(y_train, np.argmax((p1[1] + p2[1] + p3[1] + p4[1] + p5[1])/5,  axis=1)))\n",
    "print(accuracy_score(y_train, np.argmax((p1[2] + p2[2] + p3[2] + p4[2] + p5[2])/5,  axis=1)))\n",
    "print(accuracy_score(y_train, np.argmax((p1[3] + p2[3] + p3[3] + p4[3] + p5[3])/5,  axis=1)))\n",
    "print(accuracy_score(y_train, np.argmax((p1[4] + p2[4] + p3[4] + p4[4] + p5[4])/5,  axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = (p1[0] + p2[0] + p3[0] + p4[0] + p5[0])/5\n",
    "bbb = (p1[4] + p2[4] + p3[4] + p4[4] + p5[4])/5\n",
    "\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(aaa[50], 'r')\n",
    "plt.plot(bbb[50], 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(aaa.shape[0]):\n",
    "    if np.max(aaa[i]) > np.max(bbb[i]):\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_train, np.argmax(np.array(p5)[4], axis=1)))\n",
    "print(accuracy_score(y_train, np.argmax(np.mean(np.array(p5),axis=0), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1m, p2m, p3m, p4m, p5m = np.mean(p1,axis=0),np.mean(p2,axis=0),np.mean(p3,axis=0),np.mean(p4,axis=0), np.mean(p5,axis=0)\n",
    "ppm = (p1m + p2m + p3m + p4m + p5m)/5\n",
    "print(accuracy_score(y_train, np.argmax(ppm,  axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ff4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = np.concatenate(p1+p2+p3+p4+p5, axis=1)\n",
    "print(final_features.shape)\n",
    "\n",
    "LDA_final = lda_model()\n",
    "LDA_final.fit(final_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08b222",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "\n",
    "X1t = X_test[::,0:window_size,::].reshape(X_test.shape[0],window_size*X_train.shape[2])\n",
    "X2t = X_test[::,20:20+window_size,::].reshape(X_test.shape[0],window_size*X_train.shape[2])\n",
    "X3t = X_test[::,20:20+window_size,::].reshape(X_test.shape[0],window_size*X_train.shape[2])\n",
    "X4t = X_test[::,20:20+window_size,::].reshape(X_test.shape[0],window_size*X_train.shape[2])\n",
    "X5t = X_test[::,20:20+window_size,::].reshape(X_test.shape[0],window_size*X_train.shape[2])\n",
    "\n",
    "print(X1t.shape)\n",
    "\n",
    "p1t, p2t, p3t, p4t, p5t = [], [], [], [], []\n",
    "for X_slice in [X1t, X2t, X3t, X4t, X5t]:\n",
    "    for model, p_list in zip([LDA1,LDA2,LDA3,LDA4,LDA5], [p1t,p2t,p3t,p4t,p5t]):\n",
    "        p_list.append(model.predict_proba(X_slice))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa89511",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1mt, p2mt, p3mt, p4mt, p5mt = np.mean(p1t,axis=0),np.mean(p2t,axis=0),np.mean(p3t,axis=0),np.mean(p4t,axis=0), np.mean(p5t,axis=0)\n",
    "ppmt = (p1mt + p2mt + p3mt + p4mt + p5mt)/5\n",
    "print(accuracy_score(y_test, np.argmax((p1t[0] + p2t[0] + p3t[0] + p4t[0] + p5t[0])/5,  axis=1)))\n",
    "print(accuracy_score(y_test, np.argmax((p1t[1] + p2t[1] + p3t[1] + p4t[1] + p5t[1])/5,  axis=1)))\n",
    "print(accuracy_score(y_test, np.argmax((p1t[2] + p2t[2] + p3t[2] + p4t[2] + p5t[2])/5,  axis=1)))\n",
    "print(accuracy_score(y_test, np.argmax((p1t[3] + p2t[3] + p3t[3] + p4t[3] + p5t[3])/5,  axis=1)))\n",
    "print(accuracy_score(y_test, np.argmax((p1t[4] + p2t[4] + p3t[4] + p4t[4] + p5t[4])/5,  axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e19bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2, p3, p4, p5 = [], [], [], [], []\n",
    "for X_slice in [X1, X2, X3, X4, X5]:\n",
    "    for model, p_list in zip([LDA1,LDA2,LDA3,LDA4,LDA5], [p1,p2,p3,p4,p5]):\n",
    "        p_list.append(model.predict_log_proba(X_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNN = 11\n",
    "print(y_test[NNN])\n",
    "print(np.argmax(LDA.predict_proba(X_test[NNN].reshape(1, -1))))\n",
    "\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(LDA.predict_proba(X_test[NNN].reshape(1, -1))[0])\n",
    "plt.yscale('log')\n",
    "plt.ylim(1e-32, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9381557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_resp(x):\n",
    "    p1 = LDA.predict_proba(x.reshape(1, -1))\n",
    "    p2 = LDA.predict_proba(x.reshape(1, -1))    \n",
    "    p3 = LDA.predict_proba(x.reshape(1, -1))    \n",
    "    p4 = LDA.predict_proba(x.reshape(1, -1))    \n",
    "    p5 = LDA.predict_proba(x.reshape(1, -1))    \n",
    "    p_res = np.concatenate([p1,p2,p3,p4,p5])\n",
    "#     print(p_res.shape)\n",
    "    return np.mean(p_res,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de8031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef087fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit generate_resp(data_2d_resh[10].reshape(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
